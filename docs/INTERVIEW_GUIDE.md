# ðŸŽ¤ Interview Preparation Guide

## Master Your Project Story

This guide helps you present your slide transition detection project in interviews with confidence, clarity, and impact.

---

## 1. Your Project Story (The Narrative)

### The Perfect Elevator Pitch (30 seconds)

> "I built an automated system to extract high-quality slide screenshots from lecture videos. The problem was that manually extracting slides is tedious and error-prone, especially when the teacher blocks the content. I developed a hybrid solution using computer vision (histogram comparison, edge detection) combined with a machine learning classifier (Decision Tree) that achieves 97.45% accuracy. The system was validated on 14 videos with 250 manual transitions, achieving 93.6% real-world accuracy. The business impact is enabling automated note generation with timestamps for thousands of lecture recordings."

### The 2-Minute Story (Detailed)

**Problem** (15 seconds)
> "In my college, we had thousands of lecture videos, but extracting key slides for note-taking was completely manual. Students would spend hours watching videos and screenshotting slides. Teachers often stand in front of boards, blocking content. Finding the exact right moment to capture a slide is actually harder than it sounds."

**My Approach** (30 seconds)
> "I started by analyzing what makes a good slide capture: (1) timing - before the teacher changes slides, (2) quality - sharp and clear image, (3) content - slide must be fully visible without teacher blocking. I built a two-phase system. First, use fast computer vision to detect transitions - comparing histograms between frames to catch content changes, and analyzing edge density to catch layout shifts. Then, intelligently select the best frame from candidates by scoring them on fullness, quality, and occlusion."

**The Evolution** (30 seconds)
> "Initially, my rule-based approach worked okay (81% recall) but had too many false positives. So I trained a machine learning model. I collected ground truth for 14 videos - 250 manual transition timestamps. I created a dataset of 41,650 labeled frames. Then I trained a Decision Tree classifier using 4 features: content fullness, frame quality, occlusion detection, and skin ratio. The tree learned which frames are truly transitions vs noise."

**Results** (20 seconds)
> "The final system achieves 97.45% accuracy on test data and 93.6% recall on real-world validation. It processes a 1-hour video in 10-15 minutes. The machine learning model reduced false positives from 1,000+ to just 20 per video while maintaining 80%+ recall. Students can now automatically extract slides with timestamps for OCR processing."

**Key Takeaway** (10 seconds)
> "This project taught me how to combine classical computer vision with modern ML to solve real practical problems. It's not about using the fanciest algorithm - it's about understanding the problem deeply and choosing the right tool."

---

## 2. Key Improvements & Evolution

### What You Improved (Talk about this!)

#### Improvement #1: From Rule-Based to ML-Enhanced
**What Changed**:
- Started: Pure rule-based detection (histogram + edge change)
- Problem: 81% recall but 4% precision (1,000+ false positives)
- Solution: Trained Decision Tree classifier
- Result: 94% recall with 77% precision

**How to Explain in Interview**:
> "Initially, I used classical computer vision - comparing histograms and edge density between frames. It found most transitions but had lots of false alarms. I realized the real challenge wasn't detecting transitions, but filtering noise. So I collected ground truth data and trained an ML model. The Decision Tree learned patterns: full slides with sharp content are more likely transitions than partial or blurry frames. This reduced false alarms by 98% while catching more real transitions."

**Interview Question This Answers**:
- "How do you iterate and improve?"
- "Tell us about handling false positives"
- "How do you know when to use ML vs rules?"

---

#### Improvement #2: Feature Engineering (Critical!)
**What Changed**:
- Started: Just histogram difference
- Evolved: 5+ metrics computed
- Final: 4 carefully selected features

**The 4 Features**:
1. **Content Fullness** (45% importance)
   - Otsu thresholding to measure slide content
   - Differentiates full vs blank slides
   - ~2 lines of code but massive impact

2. **Frame Quality** (33% importance)
   - Laplacian variance (sharpness) + standard deviation (contrast)
   - Eliminates blurry captures
   - Inspired by video stabilization research

3. **Is Occluded** (15% importance)
   - HSV color space (H: 0-20Â°, S: 10-40%, V: 60-100%)
   - Detects teacher's skin blocking content
   - Novel application of color spaces

4. **Skin Ratio** (7% importance)
   - Percentage of frame with skin color
   - Secondary occlusion check
   - Learned it's less important than occlusion

**How to Explain in Interview**:
> "Feature engineering was crucial. I didn't just throw everything at the model - I carefully selected 4 features based on domain understanding. Content fullness was the most important (45%) because full slides have different characteristics than blank ones. Frame quality mattered (33%) because sharp images indicate good capture moments. Occlusion (15%) helped filter teacher blocking. Interestingly, skin ratio (7%) was less important, which taught me that my occlusion detection already captured most of the signal."

**Interview Question This Answers**:
- "How do you approach feature engineering?"
- "Tell us about a technical insight you gained"
- "How do you validate feature importance?"

---

#### Improvement #3: Data Collection Strategy
**What Changed**:
- Started: Testing on just 2-3 videos
- Problem: Different lectures have different characteristics
- Solution: Collected ground truth for 14 diverse videos
- Result: Validated system works across different subjects

**Diversity in Dataset**:
- 8 Chemistry videos (most data)
- 2 Physics videos
- 3 Mathematics videos
- 2 Database videos
- 1 Algorithms video
- 10 English videos + 4 Hindi videos

**How to Explain in Interview**:
> "I realized that validating on just a few videos wasn't enough. Different subjects have different board styles - chemistry has lots of equations, mathematics has diagrams, databases have tables. I deliberately collected videos across subjects and languages to ensure my system generalizes. This meant manually labeling 250 transitions across 14 videos. It was tedious, but it's exactly why my validation is robust."

**Interview Question This Answers**:
- "How do you ensure system generalization?"
- "Tell us about handling edge cases"
- "How do you validate ML models?"

---

#### Improvement #4: Handling Class Imbalance
**What Changed**:
- Problem: 97.6% non-transitions, 2.4% transitions
- Risk: Model could just predict "not transition" all the time
- Solution: Decision Tree with information gain (not accuracy-based)
- Result: 79.6% recall on minority class

**How to Explain in Interview**:
> "One challenge was extreme class imbalance - only 2.4% of frames were transitions. A naive model could get 97% accuracy by predicting everything as 'not transition'. I handled this by using information gain as the split criterion instead of accuracy, which makes the model prefer splits that separate the minority class. I also monitored recall specifically (catching transitions) rather than just overall accuracy."

**Interview Question This Answers**:
- "How do you handle imbalanced datasets?"
- "Tell us about a technical challenge"
- "How do you choose metrics beyond accuracy?"

---

#### Improvement #5: From scikit-learn to Custom Implementation
**What Changed**:
- Constraint: scikit-learn not available in environment
- Risk: Project could have stalled
- Solution: Implemented Decision Tree from scratch in numpy
- Result: Same functionality, better learning

**How to Explain in Interview**:
> "My environment didn't have scikit-learn, so instead of using a pre-built Decision Tree, I built my own from scratch using numpy. This actually strengthened my understanding of how decision trees work - the recursive splitting algorithm, information gain calculation, and handling imbalanced data. It forced me to understand, not just use."

**Interview Question This Answers**:
- "Tell us about overcoming constraints"
- "When do you build vs buy?"
- "Show us a technical deep-dive"

---

#### Improvement #6: Creating Reproducible Pipeline with Checkpoints
**What Changed**:
- Started: Everything manual, might crash mid-processing
- Problem: 14 videos Ã— 10 minutes each = 2+ hours
- Solution: Checkpoint system (`.extraction_complete` files)
- Result: Resume from last video, not restart

**How to Explain in Interview**:
> "Processing 14 videos takes 2+ hours. If something failed at video 10, restarting from video 1 is wasteful. I implemented a checkpoint system where each stage writes a marker file. If a video is already processed, the system skips it. This is a small thing but makes a big difference in practice - it's the difference between a script you can use vs a prototype you can't."

**Interview Question This Answers**:
- "Tell us about software engineering practices"
- "How do you think about production systems?"
- "What's a non-obvious improvement you made?"

---

#### Improvement #7: Validation Against Ground Truth (Not Just Metrics)
**What Changed**:
- Started: Reporting 97.45% test accuracy
- Risk: Test accuracy can be misleading
- Solution: Validate against manually labeled real-world data
- Result: More honest 93.6% recall, 95-100% frame selection accuracy

**How to Explain in Interview**:
> "I didn't just report test set metrics - I validated against real manually-labeled data. On 250 manually-marked transitions, my system achieved 93.6% recall. More importantly, the frames it selected were correct 95-100% of the time. This validation revealed the real strengths and weaknesses: the system is excellent at frame selection but has room for improvement in transition detection."

**Interview Question This Answers**:
- "How do you validate your models?"
- "Test metrics vs real-world performance?"
- "Tell us about honest evaluation"

---

### Summary of Improvements

```
Improvement 1: Rule-based â†’ ML-enhanced          (81% â†’ 94% recall)
Improvement 2: Simple detection â†’ Feature eng    (4 carefully selected features)
Improvement 3: Single video â†’ 14 diverse videos  (robust validation)
Improvement 4: Accuracy metric â†’ recall metric   (handling imbalance)
Improvement 5: External lib â†’ custom impl        (understanding > using)
Improvement 6: Manual process â†’ checkpoint sys   (reproducibility)
Improvement 7: Test metrics â†’ real validation    (honest evaluation)
```

**Meta-Improvement**: Each iteration was driven by a real problem, not by trying to add complexity for complexity's sake.

---

## 3. How to Handle Common Interview Questions

### Technical Questions

#### Q1: "Walk us through your approach"
**How to Answer**:

1. **Start with the problem** (not the solution)
   > "The challenge was extracting high-quality slide images from videos where teachers sometimes block content."

2. **Explain your initial approach**
   > "I started with a simple rule-based approach using histogram comparison and edge detection."

3. **Show your iteration**
   > "This worked 81% of the time but had too many false positives. So I collected ground truth and trained a classifier."

4. **Present the final solution**
   > "The final system uses computer vision for detection and a Decision Tree for filtering, achieving 97.45% accuracy."

5. **Discuss trade-offs**
   > "I chose Decision Tree over neural networks because it's interpretable and my dataset was only 41K samples."

**Why This Works**: Shows problem-solving, iteration, and reasoning.

---

#### Q2: "How did you get 97.45% accuracy? That seems high"
**How to Answer**:

> "Good question - and it's worth digging into. On my test set of 2,780 carefully split frames, the accuracy is 97.45%. However, when I validated on real manually-labeled transitions (250 transitions across 14 videos), the recall is 93.6%, which is more realistic. The accuracy number is high partly because 97.6% of frames are non-transitions, so even catching most negatives helps."

> "More importantly, the precision is 77% - when my system says 'this is a transition', it's right 77% of the time. This is the metric that matters for practical use."

**Why This Works**: Shows you understand the difference between test metrics and real-world performance.

---

#### Q3: "Why Decision Tree instead of neural networks?"
**How to Answer**:

> "Great question. I considered three approaches:

> 1. **Neural Network**: Better for complex patterns, but needs 100K+ samples and GPU. I only have 41K samples.

> 2. **Random Forest**: Would improve accuracy, but less interpretable.

> 3. **Decision Tree**: Interpretable (I can explain every decision), sufficient for the problem, and works well with my imbalanced dataset.

> I chose Decision Tree because interpretability mattered here - if the system fails, I need to understand why. The performance (79.6% recall on transitions) is good enough for the use case."

**Why This Works**: Shows you consider trade-offs, not just accuracy.

---

#### Q4: "How did you handle the class imbalance?"
**How to Answer**:

> "The dataset is 97.6% non-transitions and 2.4% transitions. A naive model could just predict 'not transition' and get 97% accuracy.

> I handled it in three ways:
> 1. **Split criterion**: Used information gain instead of accuracy. This makes the tree prefer splits that separate the minority class.
> 2. **Evaluation metric**: Monitored recall (catching transitions) specifically, not overall accuracy.
> 3. **Data augmentation**: Didn't do synthetic oversampling - kept real data distribution, but carefully evaluated on held-out positive examples.

> Final result: 79.6% recall on the minority class (transitions) while maintaining 98.5% specificity on negatives (non-transitions)."

**Why This Works**: Shows understanding of the problem, not just applying a technique.

---

#### Q5: "Walk us through your feature engineering"
**How to Answer**:

> "I engineered 4 features based on domain understanding:

> **Feature 1: Content Fullness (45% importance)**
> - Problem: Some slides are full of content, others mostly blank. Blanks never transition.
> - Solution: Otsu automatic threshold on grayscale â†’ percentage of dark pixels
> - Impact: Single most important feature

> **Feature 2: Frame Quality (33% importance)**
> - Problem: Blurry frames are bad captures
> - Solution: Laplacian variance (edge sharpness) + standard deviation (contrast)
> - Impact: Eliminates low-quality captures

> **Feature 3: Is Occluded (15% importance)**
> - Problem: Teachers sometimes stand in front of boards
> - Solution: HSV color space (skin detection at H: 0-20Â°, S: 10-40%, V: 60-100%)
> - Impact: Identifies blocked content

> **Feature 4: Skin Ratio (7% importance)**
> - Problem: How much of the frame is teacher?
> - Solution: Percentage of skin-colored pixels
> - Impact: Secondary signal (less important than occlusion flag)

> The key insight: Don't engineer many features. Engineer few meaningful features based on understanding the domain."

**Why This Works**: Shows domain understanding, not just ML knowledge.

---

#### Q6: "What would you do differently if you had more data?"
**How to Answer**:

> "Great question. With my current 14 videos (41K frames), Decision Tree is appropriate. With 50+ videos (200K+ frames), I would consider:

> 1. **Deep Learning**: Train a CNN with ResNet backbone. More data means the model can learn complex patterns.

> 2. **Ensemble Methods**: Combine multiple models (Random Forest, Gradient Boosting) for robustness.

> 3. **Transfer Learning**: Fine-tune a pre-trained model on lecture video domain.

> But I'd still maintain my current rule-based front-end (histogram + edge detection) because it provides interpretability for why transitions are detected."

**Why This Works**: Shows you understand scalability and when to apply different techniques.

---

#### Q7: "Describe your validation approach"
**How to Answer**:

> "I used two-level validation:

> **Level 1: Test Set Metrics**
> - Held-out 2,780 frames (15% split by video)
> - Measured accuracy, precision, recall, F1-score
> - Result: 97.45% accuracy, 77.25% precision, 79.63% recall

> **Level 2: Real-World Validation**
> - Manually labeled all 250 transitions in 14 videos
> - Measured if my system detected these within Â±5 seconds
> - Measured if the frames I selected matched the ideal frames
> - Result: 93.6% recall on real transitions, 99% frame selection accuracy

> This two-level approach shows the gap between test metrics (97.45%) and real performance (93.6%), which is important for honest evaluation."

**Why This Works**: Shows rigor and honesty about your system.

---

### Behavioral Questions

#### Q8: "Tell us about a challenge you overcame"
**How to Answer - Use STAR Method**:

**Situation**:
> "When I tested on whiteboard lectures, my system completely failed. Instant-erase whiteboards erase content so quickly that my edge-detection couldn't follow. It's like trying to detect a slide change that happens in 0.5 seconds."

**Task**:
> "I had to decide: spend weeks trying to fix instant-erase detection, or pivot to focus on what works well."

**Action**:
> "I analyzed the problem: instant-erase is fundamentally different from PPT. The erasing itself looks like a transition but isn't. Fixing this would require completely different algorithms (motion tracking, content history, etc.). Instead, I made a pragmatic decision: focus on PPT/smartboard lectures where the system works reliably. I documented this limitation clearly."

**Result**:
> "By focusing on the right problem (PPT lectures), I achieved 93.6% accuracy instead of building a mediocre solution for everything. I learned that understanding your constraints and being pragmatic about scope is as important as technical depth."

**Why This Works**: Shows problem-solving, pragmatism, and learning from failure.

---

#### Q9: "Tell us about a time you optimized something"
**How to Answer - Use STAR Method**:

**Situation**:
> "Processing 14 videos took 2+ hours. Developers don't want to wait that long every time."

**Task**:
> "Reduce iteration time for testing and debugging."

**Action**:
> "I implemented a checkpoint system. Each stage writes a marker file (`.extraction_complete`). If a video is already processed, the system skips it. This took 30 minutes to implement but saved hours in development."

**Result**:
> "Now I can test the whole pipeline in minutes instead of hours. This is a software engineering practice that matters in production systems."

**Why This Works**: Shows you think about practical concerns, not just algorithms.

---

#### Q10: "What did you learn from this project?"
**How to Answer**:

> "Three big lessons:

> **1. Problem Understanding > Algorithm Complexity**
> I could have built a fancy neural network, but understanding the problem (teachers block content, different slides have different characteristics) was more valuable than algorithm choice.

> **2. Validation is Everything**
> Test accuracy of 97.45% sounds great, but real-world validation (93.6%) tells the true story. Always validate on real data, not just test sets.

> **3. Iteration Beats Perfection**
> My first approach was 81% accurate. Second iteration was 94% accurate. Iterating based on real problems is better than trying to build the perfect system on day one."

**Why This Works**: Shows maturity and reflection.

---

### Questions About Your Specific Numbers

#### Q11: "Why 78.42% F1-score? That seems low"
**How to Answer**:

> "F1 balances precision and recall. Here's why 78% is actually good for this problem:

> - **Precision 77%**: When I detect a transition, I'm right 77% of the time. Good - few false alarms.
> - **Recall 79.6%**: I catch 80% of real transitions. Good - not missing too many.

> If I tuned for higher recall (say 95%), precision would drop to maybe 40% - thousands of false alarms. If I tuned for higher precision (say 95%), I'd miss 60% of real transitions.

> 78% F1 is the sweet spot where both matter. Plus, the rule-based system gets 81% recall anyway, so improving from there is hard."

**Why This Works**: Shows you understand metric trade-offs.

---

#### Q12: "Your model is 97.45% accurate but only 79.6% recall on transitions - how?"
**How to Answer**:

> "Great catch - this highlights why accuracy can be misleading.

> The confusion matrix is:
> - True Negatives: 2,580 (I correctly said 'not transition')
> - True Positives: 129 (I correctly said 'transition')
> - False Positives: 38 (I incorrectly said 'transition')
> - False Negatives: 33 (I incorrectly said 'not transition')

> Accuracy = (2580 + 129) / 2780 = 97.45%
> Recall = 129 / (129 + 33) = 79.6%

> The math works because there are 16x more negatives than positives. So even though I miss 33 transitions, I still get 97% overall accuracy. This is why recall is the important metric here, not accuracy."

**Why This Works**: Shows deep understanding of evaluation metrics.

---

## 4. How to Tell Your Story in an Interview

### The Flow (Strategic Order)

**1. Hook (First 30 seconds)**
> "I built an automated system to extract slide screenshots from lecture videos using computer vision and machine learning."

**2. Problem (Next 1 minute)**
> "Manually extracting slides from thousands of lecture videos is tedious. Teachers often block content. Finding the exact right moment to capture a slide is surprisingly hard."

**3. My Approach (Next 2 minutes)**
> "I started with rule-based computer vision - comparing histograms and edge density between frames. This worked 81% of the time but had too many false positives. So I collected ground truth for 14 videos and trained a Decision Tree classifier with 4 carefully engineered features."

**4. Results (Next 1 minute)**
> "97.45% test accuracy. But more importantly, 93.6% recall on real manually-labeled transitions. The system processes a 1-hour video in 10-15 minutes."

**5. Key Learning (Final 30 seconds)**
> "The biggest lesson: problem understanding beats algorithm complexity. Choosing the right features and validating on real data mattered more than trying a fancy neural network."

### Transition Techniques (Connect to Interviewer Questions)

**If they ask about technical depth**:
> "Do you want me to go deeper into the algorithms, the model training process, or the feature engineering?"

**If they ask about scale**:
> "With 14 videos and 41K frames, Decision Tree was appropriate. If we had 100K+ frames, I'd explore deep learning."

**If they ask about challenges**:
> "The hardest part was the class imbalance (97.6% non-transitions) and dealing with different board styles across subjects."

**If they ask about metrics**:
> "Test accuracy is 97.45%, but the more honest metric is 93.6% recall on real validation data. This is what matters in production."

---

## 5. Key Talking Points (Memorize These!)

### Technical Talking Points

- âœ… "Hybrid approach: fast rule-based detection + ML filtering"
- âœ… "97.45% test accuracy, 93.6% real-world recall"
- âœ… "Reduced false positives from 1,000+ to 20 per video"
- âœ… "Decision Tree (interpretable) over neural networks (black-box)"
- âœ… "4 features engineered from domain understanding"
- âœ… "Handled 40:1 class imbalance with information gain splitting"
- âœ… "Validated on 14 diverse videos, 250 manual transitions"
- âœ… "Custom implementation without scikit-learn (learned deeply)"

### Behavioral Talking Points

- âœ… "Pragmatic scope management (dropped instant-erase whiteboards)"
- âœ… "Data-driven iteration (81% â†’ 94% recall through ML)"
- âœ… "Honest evaluation (test metrics vs real-world validation)"
- âœ… "Software engineering mindset (checkpoints, reproducibility)"
- âœ… "Problem-first approach (understood domain before coding)"
- âœ… "Learning from failure (whiteboard attempt led to focus)"

---

## 6. Numbers to Know (Off-by-Heart)

```
Performance:
- Test Accuracy:        97.45%
- Test Precision:       77.25%
- Test Recall:          79.63%
- Test F1-Score:        78.42%
- Real Validation:      93.6% recall
- Ideal Frame Match:    99.0%

Dataset:
- Total Videos:         14
- Total Frames:         41,650
- Positive Samples:     1,015 (2.4%)
- Negative Samples:     40,635 (97.6%)
- Manual Transitions:   250

Model:
- Type:                 Decision Tree
- Max Depth:            15
- Feature Count:        4
- Content Fullness:     45.2% importance
- Frame Quality:        32.8% importance
- Is Occluded:          15.3% importance
- Skin Ratio:           6.7% importance

Validation:
- Precision:            77.25% (few false alarms)
- Recall:               79.63% (catches transitions)
- F1-Score:             78.42% (balanced)

Baseline Comparison:
- Baseline Recall:      81%
- Baseline Precision:   4%
- ML Model Recall:      94%
- ML Model Precision:   77%
- Improvement:          +76.9 F1 points
```

---

## 7. Interview Scenarios & Responses

### Scenario 1: "This is a nice project, but what makes it novel?"

**Response**:
> "You're right to push on this. The novelty isn't in any single technique - histogram comparison, edge detection, Decision Trees are all standard. The novelty is in the combination and the engineering:

> 1. **Problem-specific engineering**: Rather than building a general-purpose video analyzer, I engineered features specific to the lecture domain (content fullness measures slide fullness, occlusion detection targets teacher blocking).

> 2. **Pragmatic scope**: I identified that instant-erase whiteboards are fundamentally different and focused on PPT lectures. This focus allowed me to achieve 93.6% accuracy instead of a mediocre solution for everything.

> 3. **Honest evaluation**: Most papers report test metrics. I validated on real manually-labeled data and reported the gap (97.45% â†’ 93.6%).

> The real learning is that engineering solutions to real problems is as important as novel algorithms."

---

### Scenario 2: "Your class imbalance is extreme. Why not oversample or use class weights?"

**Response**:
> "Good question. I actually considered both, but decided against them:

> 1. **Oversampling**: Could help, but adds computational overhead. With information gain splitting, the tree naturally prioritizes the minority class.

> 2. **Class weights**: Would artificially inflate minority class importance during training. I prefer keeping the real distribution and carefully tuning the threshold during inference.

> Instead, I focused on the right evaluation metric (recall) rather than trying to fix the imbalance. For the production problem (catching as many real transitions as possible), 79.6% recall with 77% precision is the right trade-off."

---

### Scenario 3: "If you were building this for production, what would you change?"

**Response**:
> "Several things:

> 1. **Monitoring**: Add logging to track accuracy on new lectures. Different instructors might have different board styles.

> 2. **Retraining**: Every month, collect ground truth for new videos and retrain. The model will adapt to style drift.

> 3. **Ensemble**: Instead of single Decision Tree, use Random Forest. Slightly more complex, but more robust.

> 4. **A/B Testing**: Compare my system against manual extraction on new lectures. Real metrics matter more than test metrics.

> 5. **Feedback Loop**: Let users correct mistakes. This becomes training data for the next iteration.

> The key: a production system isn't built once - it's continuously improved based on real-world feedback."

---

### Scenario 4: "This seems specific to lectures. How would you generalize?"

**Response**:
> "Excellent insight. The approach generalizes to any video where:
> - Content changes significantly (PPT, slides, graphics)
> - You need to capture the moment before change
> - Quality matters (sharpness, not occluded)

> Examples:
> - Sports highlights (before/after goal)
> - News/documentary (scene transitions)
> - Instructional videos (before/after demonstrations)

> The generalizable principles:
> 1. **Compare consecutive frames** for change detection
> 2. **Score frames** on domain-specific quality metrics
> 3. **Validate** on real-world data, not just test metrics

> The specific features (content fullness, occlusion) would change per domain, but the framework transfers."

---

## 8. Red Flags to Avoid

### Don't Say This:

âŒ "I used the latest AI/ML technique"
- Say instead: "I chose Decision Tree for interpretability"

âŒ "97.45% accuracy means my model is near-perfect"
- Say instead: "97.45% test accuracy, but 93.6% on real validation - here's why the difference"

âŒ "I tried 10 different models and picked the best"
- Say instead: "I selected Decision Tree based on these trade-offs..."

âŒ "This problem is fully solved"
- Say instead: "Instant-erase whiteboards remain unsolved, and frame selection in crowded videos is harder"

âŒ "I implemented scikit-learn's Decision Tree better"
- Say instead: "I built from scratch to understand how it works"

---

## 9. Questions You Can Ask Back

Show your thinking by asking good questions:

1. **To understand their needs**: "Are you more concerned about recall (catching all transitions) or precision (avoiding false alarms)?"

2. **To show domain thinking**: "How would this system need to adapt if you had outdoor lectures or videos with multiple simultaneous boards?"

3. **To show engineering mindset**: "What would be the failure mode you're most worried about in production?"

4. **To show growth mindset**: "What's a metric I should track that I didn't think to implement?"

---

## 10. Final Tips

### Before the Interview

- âœ… Know your story inside-out (can explain it in 30s, 2 min, 5 min)
- âœ… Memorize key metrics (don't need to flip through papers)
- âœ… Have concrete examples ready (for each feature, algorithm, trade-off)
- âœ… Prepare your "why Decision Tree" answer word-perfect
- âœ… Practice the 2-minute elevator pitch aloud

### During the Interview

- âœ… Let them interrupt (shows confidence in your knowledge)
- âœ… When stuck, ask clarifying questions (buys thinking time)
- âœ… Use specific numbers (97.45% not "about 97%")
- âœ… Connect to their interests (listen for what they care about)
- âœ… Show enthusiasm (this is your project - own it!)

### The Attitude

> "I'm proud of this work. The system works reliably on a real problem. I iterated based on real data. I'm honest about limitations. And I'm continuously learning how to make it better."

---

**Remember**: You didn't just build a project. You solved a real problem with pragmatism, iteration, and rigor. Tell that story with confidence!

---

**Last Updated**: January 18, 2026  
**Status**: Ready for Interview Season! ðŸš€
